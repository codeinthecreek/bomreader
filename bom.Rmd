---
title: "Analysis of Data from The Australian Bureau of Meteorology (BoM)"
author: codeinthecreek
date: 2019-2020
output: html_document
---


```{r setup, include=FALSE}
# this code chunk should be run prior to any of the following R code
knitr::opts_chunk$set(echo = TRUE, eval = TRUE)
# all R code requires the tidyverse package
library(tidyverse)
```

# Outline

This document shows how to download, process and analyse data from the [Burea of Meterology](http://www.bom.gov.au), primarily in `R` using the [tidyverse](https://www.tidyverse.org/) library.


## Dependencies

### RStudio

R Markdown is intended to be run within RStudio. RStudio desktop can be downloaded for free from https://www.rstudio.com/products/rstudio/download/#download


### R Packages

There are a few packages that need to be installed to run the R code within this document. These are primarily in or adjacent to the `tidyverse` set of [packages](https://www.tidyverse.org/packages/) and consist of the following:

* The `tidyverse` set of packages;
* The `jsonlite` package for `JSON` format data;
* The `lubridate` package for data and time wrangling; and
* The `knitr` package for report generation.

Each of these can be installed with `install.packages("<name_of_package>")` at the `R` console, or via RStudio's `Tools` menu with `Install Packages`.

In all of the following `R` code snippets, it is implied that the `tidyverse` library has been loaded, as per the `setup` code chunk. This can also be done manually with the following:

```{r, warning=FALSE, message=FALSE}
library(tidyverse)
```

Other libraries that are used in only a few code chunks, are explicitly loaded in those chunks for clarity.


### Bash

Optionally, to be able to run the `bash` code chunks and set up a `cron` job for automating downloads, the `Bash` shell is required. It is included with the operating system on computers running Linux and MacOS. On Windows 10, the [windows subsystem for linux](https://docs.microsoft.com/en-us/windows/wsl/install-win10) provides `Bash`, alternatively (and the only option in older versions of windows), [Git Bash](https://gitforwindows.org/) can be used.


# Obtain Raw BoM JSON Data

Before being able to do any analysis, the data needs to be obtained. The method used here is to obtain the "Latest Weather Observations" data for the locations of interest. This data only covers 72 hours, so it needs to be downloaded periodically to build up sufficient data for identifying trends over time, comparing readings between locations or seasons. The best way of doing this is via a `cron` job, which automates the download on a regular interval - every second day (allowing overlap to ensure no readings are missed) or third day (no overlap, and depending on timing, may miss a single reading if the reading is published late) is sufficient.


## Finding the URL of a location's observation data

To download data for a location, the URL of the data is required. The URLs for the raw JSON data take the form
```
http://www.bom.gov.au/products/<state_code>/<state_code>.<location_code>.shtml
```

These codes can be found via the BoM [website](http://www.bom.gov.au) by going to the page containing the relevant state of Australia's latest observations (currently accessible from the front page via the near top right menu links per state, each with a dropdown from which`All observations` or `All <Name_of_State> observations` can be selected. This will take you to a page with a list of stations and their latest readings. For example [Latest Weather Observations for Queensland](http://www.bom.gov.au/qld/observations/qldall.shtml). Then clicking on the station name for the location of interest.

For example, `Townsville` has the URL http://www.bom.gov.au/products/IDQ60801/IDQ60801.94294.shtml.

The product code for the state is the first part of the filename (`IDQ60801` in the example), and the location code is the number after the period (`94294` in the example).


## Download json data from BoM using Bash and a Cron job

*Note*: for downloading BoM data within RStudio use the R version provided in the following section.

The following `Bash` shell code extract (taken from [getweatherobs.sh](https://github.com/codeinthecreek/bomreader) with minor modifications to be a standalone script) shows how to download the location data (obtained as a `JSON` file) from the web into a file for processing. It utilises associative arrays to lookup the codes required to construct the location's URL.

The following code should be modified to include the locations of interest in `locations`, with their states added to `locationstates` and codes specified in `locationcodes`. This should be saved as `download_location_data.sh` in a directory on the `$PATH` where it can be run no matter what the current working directory is. Here we'll use the user's `$HOME/bin` directory as per the `crontab` below.

```{bash, eval=FALSE}
# space delimited list of locations
locations="Townsville_AP Townsville_MS Darwin_AP"

declare -A locationstates
locationstates=( \
    ["Townsville_AP"]="QLD" \
    ["Townsville_MS"]="QLD" \
    ["Darwin_AP"]="NT" \
)

declare -A locationcodes
locationcodes=( \
    ["Townsville_AP"]="94294" \
    ["Townsville_MS"]="94272" \
    ["Darwin_AP"]="94120" \
)

declare -A statecodes
statecodes=( \
    ["QLD"]="IDQ60801/IDQ60801" \
    ["NT"]="IDD60801/IDD60801" \
    ["WA"]="IDW60801/IDD60801" \
    ["SA"]="IDS60801/IDD60801" \
    ["NSW"]="IDN60801/IDD60801" \
    ["VIC"]="IDV60801/IDD60801" \
    ["TAS"]="IDT60801/IDD60801"
)

for loc in $locations; do
    state=${locationstates[$loc]}
    scode=${statecodes[$state]}
    lcode=${locationcodes[$loc]}

    url="http://www.bom.gov.au/fwo/${scode}.${lcode}.json"
    fname="${loc}-$(/bin/date +\%F).json"

    echo >&2 "location $loc in state $state with state code $scode and location code $lcode"
    echo >&2 "retrieving URL $url"
    echo >&2 "writing to file $fname"

    curl "$url" 2>/dev/null > $fname
done

echo >&2 "Done!"
```


Once this is done, a data directory for the downloads should be created. Here, we'll use `$HOME/bom_data`.

To set up a `cron` job to download BoM data every 48 hours (ensuring full data coverage as BoM data covers 72 hours), enter the following at the command line to edit the crontab
```bash
crontab -e
```

Assuming an empty crontab, and the script and data directory given above, paste the following into the crontab, replacing `username` with your username. Note empty last line.
```bash
MAILTO=""
PATH=/home/username/bin:/usr/local/bin:/usr/bin:/usr/sbin:/bin:/sbin
#  min  hr  dom  mon  dow   command
    10  22  */2   *    *    (cd $HOME/bom_data/; download_location_data.sh)

```

Save and exit the editor.

The cronjob will then run at 10:10 pm (allowing 10 minutes leeway for the 10pm update) every odd day of the month (so with months having 31 days, it will be run on the last day of the month and then 24 hours later on the first day of the new month), downloading the `JSON` data for each of the specified locations.


## Download data from BoM using R.

As opposed to using `bash` and a `cron` job, the BoM `JSON` data can be downloaded directly with `R`, as follows.


### Setup location tables

The location code tables need to be set up prior to downloading data. These are used to generate the required download URL.

```{r, warning=FALSE}
loccodes <- tribble(
  ~location, ~state, ~lcode,
  "Brisbane_AP", "QLD", "94578",
  "Rockhampton", "QLD", "94374",
  "Mackay_AP", "QLD", "95367",
  "Proserpine", "QLD", "94365",
  "Bowen_AP", "QLD", "94383",
  "Townsville_AP", "QLD", "94294",
  "Townsville_MS", "QLD", "94272",
  "Innisfail_AP", "QLD", "94280",
  "Cairns", "QLD", "94287",
  "Cairns_RC", "QLD", "94288",
  "Mareeba", "QLD", "95286",
  "Cooktown", "QLD", "95283",
  "Darwin_AP", "NT", "94120"
)

statecodes <- tribble(
  ~state, ~scode,
  "QLD", "IDQ60801/IDQ60801",
  "NT", "IDD60801/IDD60801",
  "WA", "IDW60801/IDD60801",
  "SA", "IDS60801/IDD60801",
  "NSW", "IDN60801/IDD60801",
  "VIC", "IDV60801/IDD60801",
  "TAS", "IDT60801/IDD60801"
)

# specify 'by' argument to prevent join() printing field it chooses to join on
loctable <- loccodes %>% inner_join(statecodes, by="state")
```


### Download latest 72 hours of data from BoM for 1 or more locations {#multidl}

Before running the following, specify the location(s) of interest in the `loc` list, and ensure the locations have an entry in the `loccodes` table (above). Note that `download_dir` should be the same directory used later in section [Read all specified JSON files from a directory](#readdir).

```{r, eval=FALSE, warning=FALSE}
loc <- c("Brisbane_AP", "Townsville_AP", "Cairns_RC", "Darwin_AP")

download_dir="./tmp"

# get the codes for the selected locations
codes <- loctable %>%
  filter(location %in% loc) %>%
  mutate(url = paste0("http://www.bom.gov.au/fwo/", scode, ".", lcode, ".json")) %>%
  mutate(destfile = sprintf("%s/%s-%s.json", download_dir, location, Sys.Date()))

# note in the following selected column order and names are important
# (sprintf needs fmt first; download.file needs url and destfile)
codes %>% mutate(fmt="downloading %s => %s") %>% select(fmt, url, destfile) %>% pmap(sprintf)
codes %>% select(url, destfile) %>% pmap(download.file)
```


#### Single location download {#singledl}

The following shows how to download a single observation data file from BoM. The result will be a downloaded `JSON` file, named by location and date, with the filename specified in the variable `fname`.

*Note*: this is for illustrative purposes only, and should be used in conjunction with the later section [Process a single JSON file](#readsingle).

```{r, eval=FALSE}
loc <- "Townsville_MS"

# get the codes for the selected location, return a dataframe
codes <- loctable %>% filter(location==loc)# %>%

# extract code from the first cell (should only be 1) of each field
# note that [] extracts a list, [[]] extracts an element within the list
#lcode <- codes$lcode[[1]]
#scode <- codes$scode[[1]]
# the above code is deprecated - instead use pull() from lplyr package
lcode <- codes %>% pull(lcode)
scode <- codes %>% pull(scode)

url <- paste0("http://www.bom.gov.au/fwo/", scode, ".", lcode, ".json")
#cat("retrieving URL ", url, "\n", sep="") # sep="" to match message()
message("retrieving URL ", url, "\n")

fname <- sprintf("%s-%s.json", loc, Sys.Date())
#cat("downloading to file ", fname, "\n", sep="") # to stdout
message("downloading to file ", fname, "\n") # to stderr
download.file(url, destfile=fname)
message("Done!\n")
```


# Read BoM data from JSON file(s) to a dataframe and pre-process

Once the BoM data has been obtained as one or more `JSON` files, it can then be read into `R` as a dataframe.


## Read all specified JSON files from a directory {#readdir}

The following code will process all the files matching the `filename_pattern` regex within the directory specified by `data_dir`, which should usually be the same location as `download_dir` specified in the code chunk in the earlier section [Download latest 72 hours of data from BoM for 1 or more locations](#multidl).

```{r, warning=FALSE, message=FALSE}
library(jsonlite)

#data_dir <- "./data"
data_dir <- "./tmp/201907"
#data_dir <- "./tmp/201912"

# some example filename patterns - modify to suit
#filename_pattern <- "(Townsville_AP|Townsville_MS|Cairns|Cairns_RC|Cooktown|Bowen_AP|Mackay_AP|Rockhampton|Brisbane_AP|Darwin_AP)-2019-07-.*\\.json$"
#filename_pattern <- "(Townsville_AP|Cairns_RC)-2019-.*\\.json$"
#filename_pattern <- "Townsville_(MS|AP)-2019-07-.*\\.json$"
#filename_pattern <- "Townsville_AP-2019-(01|02)-.*\\.json$"
filename_pattern <- "*\\.json$"

files <- list.files(data_dir, pattern =filename_pattern, full.name = TRUE)

# read the observations:data contents of all specified JSON files into a dataframe
# and remove duplicates due to overlapping observation data
observation_data <- files %>%
       map_df(~fromJSON(.)$observations$data) %>%
       distinct()


```


### Process a single JSON file {#readsingle}

The following shows how to read a single JSON file (with the filename specified in `fname`) that was downloaded from BoM in section [Single location download](#singledl), into a dataframe for analysis.

*Note*: this is for illustrative purposes only.

```{r, eval=FALSE}
library(jsonlite)

cat("reading observation data as JSON from file: ", fname, "\n")
rawj <- fromJSON(fname)
# this will contain: observations -> notice[], header[], data[]
# extract just the data as a dataframe
observation_data <- rawj$observations$data
```


## Convert the string based date and time to datetime type

The string based date and time string contained in `local_date_time_full` needs to be converted to a `datetime` object. As we are only concerned with the local time, and not comparing time across timezones, the timezone information can either be ignored (the simple way shown first), or utilised (more complex, as shown after). Only one of these two approaches needs to be run.


### Ignore timezone

The simplest way to deal with date times is to ignore the timezone, as we only care about the local time.

```{r, eval=FALSE}
library(lubridate)

# convert time to a datetime
obs_data <- observation_data %>%
  mutate(date_time = ymd_hms(local_date_time_full))
```


### Specify timezone for each location

Optionally, we can set the timezone to be that of the state the location is based in.

```{r, warning=FALSE, message=FALSE}
library(lubridate)

# complex case - various timezones
# set timezone based on first 3 chars of history_product code:
#   IDD = NT, IDQ = Qld, IDW = WA, IDS = SA, IDV = Vic, IDN = NSW, IDT = Tas

prodtotimezone <- tribble(
  ~pcode, ~timezone,
  "IDD", "Australia/Darwin",
  "IDQ", "Australia/Brisbane",
  "IDW", "Australia/Perth",
  "IDN", "Australia/Sydney",
  "IDS", "Australia/Adelaide",
  "IDV", "Australia/Melbourne",
  "IDT", "Australia/Hobart"
)

# create the 3 char pcode field to join with the above table to get the timezone
obs_data <- observation_data %>%
  mutate(pcode = substr(history_product, 1, 3)) %>%
  inner_join(prodtotimezone, by="pcode") %>%
  select(-pcode) # no longer needed after the join

# then set the time using the timezone information
# the following seems to work - set datetime with default timezone,
# then group by timezone and set the timezone
obs_data <- obs_data %>%
  mutate(date_time = ymd_hms(local_date_time_full)) %>%
  group_by(timezone) %>%
  mutate(date_time = with_tz(date_time, tzone=first(timezone))) %>%
  ungroup()
```


### Round times to nearest half hour

*TODO*

Note: Most data is to the closest half hour, but
      occasionally there are readings that have odd times.
      These make graphing, in particular, less effective.
      The solution is to round these to the closest half hour
      that is missing.


# Analyse Bom data for location-based temperature information

With the BoM data loaded and datetime preprocessed it can now be analysed.

## Extract relevant information

The first step is to extract only the relevant information for temperature-based queries, and provide "working" dates and times for ease of use in the following sections.

```{r}
# separate temperature data (air_temp) and drop rows with missing values
# so that don't end up with NA's breaking stats, then
temp_obs <- obs_data %>%
  select(name, date_time, air_temp, apparent_t, rel_hum, wind_dir, wind_spd_kmh) %>%
  drop_na() %>%
  mutate(whr = hour(date_time)) %>%
  mutate(wymd = as_date(date_time)) %>%
  mutate(wtime = sprintf("%02d:%02d", hour(date_time), minute(date_time)))
```


### Optionally filter temperature observations for locations of interest

```{r}
temp_obs <- temp_obs %>%
  filter(name %in% c("Brisbane Airport", "Townsville", "Cairns Racecourse", "Darwin Airport"))
```


### Remove non-full day data

To prevent erroneous maximum and minimums for days that only contain the last few hours of data (assuming data is downloaded around 10pm).

*TODO*


## Find mean temperature for locations of interest over period covered by data

```{r}
# mean daily temp per location of interest
daily_mean_loc <- temp_obs %>%
  group_by(name, wymd) %>%
  summarise(meantemp = mean(air_temp))
```

#### Graph daily mean temperature over date range

```{r}
ggplot(daily_mean_loc) +
  geom_point(aes(x=wymd, y=meantemp, color=name, shape=name))
```


## Find hottest and coldest times of day

```{r}
# find hottest time of day, and it's avg temp, per location of interest
# note that time intervals are half hourly, so don't need to bin times

# TODO: add wind_dir to test what most common wind_dir is during
# hottest and coldest times

# first get hottest times and their frequency - for graphing
hottest_times_loc <- temp_obs %>%
  group_by(name, wymd) %>% # for each location and day
  summarise(max_temp = max(air_temp), hottest_time = wtime[which.max(air_temp)]) %>% # get hottest time and temp
  group_by(name, hottest_time) %>% # group by hottest time to get
  summarise(avg_max=mean(max_temp), freq=n()) # count of how often it is hottest time and average max at that time

# then find the hottest time per location
hottest_time_loc <- hottest_times_loc %>%
  group_by(name) %>% # then per location
  top_n(1, freq) %>% # keep time occuring most often
  group_by(name, freq) %>% # OPTIONAL tie break to keep only
  filter(avg_max==max(avg_max)) # the hottest of these times

coldest_times_loc <- temp_obs %>% 
  group_by(name, wymd) %>%
  summarise(min_temp = min(air_temp), coldest_time = wtime[which.min(air_temp)]) %>%
  group_by(name, coldest_time) %>%
  summarise(avg_min=mean(min_temp), freq=n())

coldest_time_loc <- coldest_times_loc %>%
  group_by(name) %>%
  top_n(1, freq) %>% 
  group_by(name, freq) %>% # OPTIONAL tie break on lowest min
  filter(avg_min==min(avg_min))
```

### Prepare dataframes for graphing

Prior to graphing time based information, the string-based time needs to be converted to `POSIXct` format. The date isn't shown in the graphs, so we leave the default of today. Note that we set the time zone to `UTC` here and use it in the graphs to prevent unintended timeshifts in displayed times.

```{r}
hottest_times_posix <- hottest_times_loc %>% mutate(posix_time=as.POSIXct(hottest_time, format = "%H:%M", tz="UTC"))

coldest_times_posix <- coldest_times_loc %>% mutate(posix_time=as.POSIXct(coldest_time, format = "%H:%M", tz="UTC"))
```


#### Display bar Graph of hottest times of day

```{r, warning=FALSE, message=FALSE}
library(scales)

# focus on relevant times - easier to see,
# and removes some erroneous data from evenings (incomplete days)
limits_morning_to_eve=c(
  as.POSIXct("10:00", format = "%H:%M", tz="UTC"),
  as.POSIXct("18:00", format = "%H:%M", tz="UTC"))

ggplot(hottest_times_posix) +
  geom_col(mapping=aes(x=posix_time, y=avg_max, fill=freq), position=position_dodge()) +
  #scale_x_discrete(name="hottest_times", breaks=map(seq(0,23), ~ sprintf("%0.2d:00", .))) +
  scale_x_datetime(
    breaks=date_breaks("1 hours"),
    limits=limits_morning_to_eve, # easier to see
    labels=date_format("%H:%M", tz="UTC"), # same tz as when created
    expand = c(0, 0) # deals with not having data at every time
    ) +
  theme(axis.text.x= element_text(angle=90)) +
  xlab("hottest times") +
  ylab("average maximum") +
  facet_wrap( ~ name, nrow=2)
```


#### Display bar Graph of coldest times of day

```{r, warning=FALSE, message=FALSE}
library(scales)

# this doesn't work - need factors
#breaks_noon_to_noon=c(
#  seq(as.POSIXct("12:00", format = "%H:%M", tz="UTC"),
#      as.POSIXct("24:00", format = "%H:%M", tz="UTC"),
#      "1 hours"),
#  seq(as.POSIXct("00:00", format = "%H:%M", tz="UTC"),
#      as.POSIXct("12:00", format = "%H:%M", tz="UTC"),
#      "1 hours")
#)

# this prevents huge gap in most of x axis,
# and removes some erroneous data from evenings (incomplete days)
limits_midnight_to_morning=c(
  as.POSIXct("00:00", format = "%H:%M", tz="UTC"),
  as.POSIXct("08:00", format = "%H:%M", tz="UTC"))

ggplot(coldest_times_posix) +
  geom_col(mapping=aes(x=posix_time, y=avg_min, fill=freq), position=position_dodge()) +
  scale_x_datetime(
    breaks=date_breaks("1 hours"),
    #breaks=breaks_noon_to_noon, # doesn't work
    limits=limits_midnight_to_morning, # easier to see
    labels=date_format("%H:%M", tz="UTC"), # same tz as when created
    expand = c(0, 0) # deals with not having data at every time
    ) +
  theme(axis.text.x=element_text(angle=90)) +
  xlab("coldest times") +
  ylab("average minimum") +
  facet_wrap( ~ name, nrow=2)
```


#### Show the (usually) hottest time of day per location as a table

```{r, warning=FALSE, message=FALSE}
library(knitr)
kable(hottest_time_loc, digits=2)
```


#### Show the (usually) coldest time of day per location as a table

```{r, warning=FALSE, message=FALSE}
library(knitr)
kable(coldest_time_loc, digits=2)
```




## Analyse BoM data for location-based time of day temperature information

In this section we'll analyse the temperature to provide typical temperatures for each time of day. night, morning, day and evening over the time period contained in the provided data. This gives a more accurate reflection of weather & climate (for comparison), as opposed to just maximum and minimum temperatures.

The periods of the day are defined as

* night: 10 PM - 6 AM
* morning: 6 AM - 10 AM
* daytime: 10 AM - 6 PM
* evening: 6 PM - 10 PM

such that the times of rapidly rising and falling temperatures (morning, evening) are separated from times of more stable temperatures (night, daytime), and time periods correspond roughly with perceptions of these times of day.


### Normalise dates and set time of day

```{r}
# normalise dates for overnight observations (10pm-6am)
# so that 10pm-11:59pm belong to the following day
# and calculate time of day as follows
# morning: 06:00 - 09:59
# daytime: 10:00 - 17:59
# evening: 18:00 - 21:59
# overnight: 22:00 - 05:59

tod_temp_obs <- temp_obs %>%
  mutate(wtod = case_when(
         whr >= 6 & whr < 10 ~ "1-morn",
         whr >= 10 & whr < 18 ~ "2-day",
         whr >= 18 & whr < 22 ~ "3-eve",
         whr >= 22 | whr < 6 ~ "0-night"
  )) %>%
  mutate(wdate = if_else(whr >= 22,
                         as_date(date_time + period(1, "day")),
                         as_date(date_time))
         ) %>%
  mutate(wmonth = month(wdate)) %>%
  mutate(wyear = year(wdate))
```


#### Smooth graph of location temperature over date range, per time of day

```{r}
ggplot(tod_temp_obs) +
  geom_smooth(aes(x=wdate, y=air_temp, color=name)) +
  facet_wrap( ~ wtod, nrow=2)
```


### Extract location-based time of day temperature information

```{r}
# group observations by (location and) time of day for each day
# and calculate for temp: mean, max, min, spread (max-min)
daily_obs_by_tod <- tod_temp_obs %>%
  group_by(name, wdate, wtod) %>%
  summarise(avt = mean(air_temp),
            maxt = max(air_temp),
            mint = min(air_temp),
            spreadt = max(air_temp) - min(air_temp)
            )
```


#### Graph daily time of day average temperature over date range

```{r}
ggplot(daily_obs_by_tod) +
  geom_point(aes(x=wdate, y=avt, color=name, shape=name)) +
  facet_wrap( ~ wtod, nrow=2)
```


#### Boxplot locations per time of day

```{r}
ggplot(daily_obs_by_tod) +
  geom_boxplot(mapping = aes(x=name, y=avt, color=name)) +
  coord_flip() +
  facet_wrap( ~ wtod, nrow=2)
```


#### Boxplot time of day per location

```{r}
ggplot(daily_obs_by_tod) +
  geom_boxplot(mapping = aes(x=wtod, y=avt, color=name)) +
  coord_flip() +
  facet_wrap( ~ name, nrow=2)
```


### Calculate min, max, average and spread of temps over all observations for each time of day

```{r}

# for each location calculate average of TOD mean, max, min, spread (max-min)
summary_all_obs_by_tod <- tod_temp_obs %>%
  group_by(name, wtod) %>%
  summarise(avt = mean(air_temp),
            maxt = max(air_temp),
            mint = min(air_temp),
            spreadt = max(air_temp) - min(air_temp)
            )
```


#### Display time of day temperature stats as a table

```{r, warning=FALSE, message=FALSE}
library(knitr)
#View(summary_all_obs_by_tod)
kable(summary_all_obs_by_tod, digits=2)
```


### Calculate min, max, mean and spread of typical TOD temps

```{r, warning=FALSE, message=FALSE}
library(knitr)

# summarise and typical (average) temp by time of day
summary_avt_by_tod <- daily_obs_by_tod %>%
  group_by(name, wtod) %>%
  summarise(mean_avt = mean(avt),
            max_avt = max(avt),
            min_avt = min(avt),
            spread_avt = max(avt) - min(avt)
            )
```


#### Display a bar graph of the typical (average) temp by time of day

```{r}
ggplot(summary_avt_by_tod) +
  geom_col(mapping=aes(x=wtod, y=mean_avt, fill=name), position=position_dodge())
```


#### Display typical (average) temp as a table

```{r, warning=FALSE, message=FALSE}
library(knitr)
# for each loc provide typical (average) temp for each time of day for viewing
typical_tod_temp_by_loc <- summary_avt_by_tod %>%
  select(name, wtod, mean_avt) %>%
  spread(key = wtod, value = mean_avt) %>%
  rename("night" = "0-night", "morn" = "1-morn", "day" = "2-day", "eve" = "3-eve") %>%
  group_by(name) %>%
  summarise(
    morning = round(morn, digits=1),
    daytime = round(day, digits=1),
    evening = round(eve, digits=1),
    overnight = round(night, digits=1),
    meantemp = round(mean(c(morn, day, eve, night)), digits=1)
    ) %>%
  arrange(meantemp)

#head(typical_tod_temp_by_loc)
kable(typical_tod_temp_by_loc, digits=2)
```



### Compare monthly temperature data

*Note* This is a work in progress.

```{r}
# Group time of day observations by month
monthly_obs_by_tod <- tod_temp_obs %>%
  group_by(name, wmonth, wtod) %>%
  summarise(avt = mean(air_temp),
            maxt = max(air_temp),
            mint = min(air_temp),
            spreadt = max(air_temp) - min(air_temp)
            )
```


#### Plot typical location temperature over months, per time of day

```{r}
ggplot(monthly_obs_by_tod) +
  geom_point(aes(x=wmonth, y=avt, color=name, shape=name)) +
  scale_x_discrete(name="wmonth", limits=1:12) +
  facet_wrap( ~ wtod, nrow=2)
``` 


#### Box plot typical location temperature per months and time of day

```{r}
ggplot(monthly_obs_by_tod) +
  geom_boxplot(mapping = aes(x=name, y=avt, color=name)) +
  coord_flip() +
  facet_grid(wmonth ~ wtod)
```


#### Compare monthly time of day temperatures per location as plot

```{r}
ggplot(monthly_obs_by_tod) +
  geom_point(aes(x=wmonth, y=avt, color=wtod, shape=wtod)) +
  scale_x_discrete(name="wmonth", limits=1:12) +
  facet_wrap( ~ name, nrow=2)
```


#### Compare monthly time of day average temperature for locations as a bar graph

```{r}
ggplot(monthly_obs_by_tod) +
  #geom_bar(mapping=aes(x=wmonth, y=avt, fill=name), stat="identity", position=position_dodge()) + # equiv to geom_col w/out stat="identity"
  geom_col(mapping=aes(x=wmonth, y=avt, fill=name), position=position_dodge()) +
  scale_x_discrete(name="wmonth", limits=1:12) +
  facet_wrap( ~ wtod, nrow=2)
```

### Compare yearly temperature data

*TODO*

```{r, eval=FALSE, echo=FALSE}
# Group time of day observations by year
yearly_obs_by_tod <- tod_temp_obs %>%
  group_by(name, wyear, wtod) %>%
  summarise(avt = mean(air_temp),
            maxt = max(air_temp),
            mint = min(air_temp),
            spreadt = max(air_temp) - min(air_temp)
            )
```


## Other Climate Statistics

### Rainfall Statistics

```{r, eval=FALSE, echo=FALSE}
library(lubridate)

# separate rainfall data (rain_trace)
# convert rain_trace to double (as by default it turns out as a string)
# and drop rows with missing values (as.numeric() converts '-' to NA)
# so that don't end up with NA's breaking stats, then order by date_time for each location (name)
# and normalise dates to 9am when rain trace ends

rain_obs <- obs_data %>%
  select(name, date_time, rel_hum, rain_trace) %>%
  mutate(rain_trace = as.numeric(rain_trace)) %>%
  drop_na() %>% # or use: filter(!is.na(rain_trace))
  arrange(name, date_time) %>%
  mutate(wtime = sprintf("%02d:%02d", hour(date_time), minute(date_time))) %>%
  mutate(whr = hour(date_time)) %>%
  mutate(wmin= minute(date_time)) %>%
  mutate(wymd = if_else((whr < 9 | (whr == 9 & wmin == 0)),
                         as_date(date_time - period(1, "day")),
                         as_date(date_time))
         )

# as rain_trace is cummulative, get difference between readings for rain in that time period
# note that this requires the readings to be in order (per location)
rain_obs <- rain_obs %>%
  group_by(name, wymd) %>%
  mutate(wrain = rain_trace - lag(rain_trace))

```

#### Total Daily Rain by Location

```{r}
# total daily rain per location of interest
daily_rain_loc <- rain_obs %>%
  filter(name %in% c("Mackay Airport", "Townsville", "Cairns", "Cooktown", "Darwin Airport")) %>%
  filter(whr==9 & wmin==0) # rain_trace at 09:00 is total for previous 24 hours
```

```{r}
# filter for days where rainfall exceeds a threshold (eg 2.5mm)
daily_rain_loc_thresh <- daily_rain_loc %>%
  filter(rain_trace >= 2.5)

ggplot(daily_rain_loc_thresh) +
  geom_point(aes(x=wymd, y=rain_trace, color=name, shape=name))
```



#### rain by hour of day over date range for each location

```{r}
# TODO
```


#### most frequent time of day (hour) for rain / wettest time of day

```{r}
# TODO
```


### Wind Direction and Speed Statistics

*TODO*

```{r, eval=FALSE, echo=FALSE}
# select fields of interest
wind_obs <- obs_data %>%
  select(name, date_time, wind_dir, wind_spd_kmh) %>%
  drop_na() %>%
  mutate(whr = hour(date_time)) %>%
  mutate(wymd = as_date(date_time)) %>%
  mutate(wtime = sprintf("%02d:%02d", hour(date_time), minute(date_time)))

# by day, loc for each hour of the day
# most common wind direction and speed
# wind_dir ordered by strength (speed) and average speed
# 
# 
# TODO
```



# References

* [R for Data Science](https://r4ds.had.co.nz)
* [Manipulating data tables with dplyr](https://mgimond.github.io/ES218/Week03a.html)
* [7 Most Practically Useful Operations When Wrangling with Text Data in R](https://blog.exploratory.io/7-most-practically-useful-operations-when-wrangling-with-text-data-in-r-7654bd9d1a0c)
* [5 Most Practically Useful Operations When Working with Date and Time in R](https://blog.exploratory.io/5-most-practically-useful-operations-when-working-with-date-and-time-in-r-9f9eb8a17465)
* [`lubridate` package](https://lubridate.tidyverse.org/)
* `purrr` [`map`](https://hookedondata.org/going-off-the-map/)
* `purrr` [Lessons and Examples](https://jennybc.github.io/purrr-tutorial/index.html)
* `ggplot2` [Essentials](http://www.sthda.com/english/wiki/ggplot2-essentials)
* `ggplot` [Axis tick marks and labels](http://www.sthda.com/english/wiki/ggplot2-axis-ticks-a-guide-to-customize-tick-marks-and-labels)
* `ggplot` [Plotting Dates, Hours and Minutes](https://learnr.wordpress.com/2010/02/25/ggplot2-plotting-dates-hours-and-minutes/)
* [R Markdown](https://ourcodingclub.github.io/2016/11/24/rmarkdown-1.html)

# Related Work

* [`bomrang` package](https://cran.r-project.org/web/packages/bomrang/)

